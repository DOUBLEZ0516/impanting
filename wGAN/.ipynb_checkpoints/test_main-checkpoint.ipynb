{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from utils import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202599\n",
      "tr: 182339  ts: 20260  tt: 202599\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "z_dim = 100\n",
    "data_dir = \"../../data/\"\n",
    "data_name = \"img_align_celeba\"\n",
    "print(len(glob.glob(os.path.join(data_dir+data_name + '/*.jpg'))))\n",
    "train_iter,train_init,test_iter,test_init = data_feeder2([data_dir+data_name],\"jpg\",5,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 218, 178, 3)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(train_init)\n",
    "test_face_train_data = sess.run(train_iter.get_next())\n",
    "print(test_face_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit = critic(test_face_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess.run(tf.global_variables_initializer())\n",
    "# lg = sess.run(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dist = tf.contrib.distributions.Normal(0.,1.)\n",
    "z = noise_dist.sample((batch_size, z_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen4,gen3,gen2,gen1 = generator(z)\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# ge4,ge3,ge2,ge1 = sess.run([gen4,gen3,gen2,gen1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ge4.shape,ge3.shape,ge2.shape,ge1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(gp=False):\n",
    "    noise_dist = tf.contrib.distributions.Normal(0.,1.)\n",
    "    z = noise_dist.sample((batch_size, z_dim))\n",
    "    with tf.variable_scope('generator'):\n",
    "        gen_data = generator(z)\n",
    "    real_data = tf.placeholder(\n",
    "        dtype=tf.float32,\n",
    "        shape=((batch_size,218,178,3)))\n",
    "    true_logit = critic(real_data)\n",
    "    fake_logit = critic(gen_data, reuse=True)\n",
    "    c_loss = tf.reduce_mean(fake_logit-true_logit)\n",
    "    if gp == True:\n",
    "        gp = False\n",
    "    g_loss = tf.reduce_mean(-fake_logit)\n",
    "    \n",
    "    g_loss_sum = tf.summary.scalar(\"g_loss\", g_loss)\n",
    "    c_loss_sum = tf.summary.scalar(\"c_loss\", c_loss)\n",
    "    img_sum = tf.summary.image(\"gen\", gen_data, max_outputs=10)\n",
    "    theta_g = tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "    theta_c = tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope='critic')\n",
    "    opt_g = tf.train.RMSPropOptimizer(1e-3).minimize(g_loss, var_list = theta_g)\n",
    "    opt_c = tf.train.RMSPropOptimizer(1e-3).minimize(c_loss, var_list = theta_c)\n",
    "    if gp==False:\n",
    "        clipped_var_c = [tf.assign(var, tf.clip_by_value(var, -0.01, 0.01)) for var in theta_c]\n",
    "        # merge the clip operations on critic variables\n",
    "        with tf.control_dependencies([opt_c]):\n",
    "            opt_c = tf.tuple(clipped_var_c)\n",
    "    return opt_g,opt_c,real_data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_dir = \"logs\",epoch = 100,):\n",
    "    opt_g, opt_c, real_data = build_graph()\n",
    "    saver = tf.train.Saver()\n",
    "    merged_all = tf.summary.merge_all()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "        for e in range(epoch):\n",
    "            sess.run(train_init)\n",
    "            next_ele = train_iter.get_next()\n",
    "            for step in range(int(252559/batch_size)):\n",
    "                i = e*int(252559/batch_size)+step\n",
    "                next_batch = sess.run(next_ele)\n",
    "                if i < 25 or i % 500 == 0:\n",
    "                    citers = 100\n",
    "                else:\n",
    "                    citers = Citers\n",
    "                for j in range(citers):\n",
    "                    next_batch = sess.run(next_ele)\n",
    "                    if i % 100 == 99 and j == 0:\n",
    "                        run_options = tf.RunOptions(\n",
    "                            trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                        run_metadata = tf.RunMetadata()\n",
    "                        _, merged = sess.run([opt_c, merged_all], feed_dict=feed_dict,\n",
    "                                             options=run_options, run_metadata=run_metadata)\n",
    "                        summary_writer.add_summary(merged, i)\n",
    "                        summary_writer.add_run_metadata(\n",
    "                            run_metadata, 'critic_metadata {}'.format(i), i)\n",
    "                    else:\n",
    "                        sess.run(opt_c, feed_dict={real_data:next_batch})\n",
    "                next_batch = sess.run(next_ele)\n",
    "                if i % 100 == 99:\n",
    "                    _, merged = sess.run([opt_g, merged_all], feed_dict=feed_dict,\n",
    "                         options=run_options, run_metadata=run_metadata)\n",
    "                    summary_writer.add_summary(merged, i)\n",
    "                    summary_writer.add_run_metadata(\n",
    "                        run_metadata, 'generator_metadata {}'.format(i), i)\n",
    "                else:\n",
    "                    sess.run(opt_g, feed_dict={real_data:next_batch})                \n",
    "                if i % 1000 == 999:\n",
    "                    saver.save(sess, os.path.join(\n",
    "                        ckpt_dir, \"model.ckpt\"), global_step=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/izaac/anaconda3/envs/sourcedl/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py:356: UserWarning: An unusually high number of `Iterator.get_next()` calls was detected. This often indicates that `Iterator.get_next()` is being called inside a training loop, which will cause gradual slowdown and eventual resource exhaustion. If this is the case, restructure your code to call `next_element = iterator.get_next()` once outside the loop, and use `next_element` as the input to some computation that is invoked inside the loop.\n",
      "  warnings.warn(GET_NEXT_CALL_WARNING_MESSAGE)\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
